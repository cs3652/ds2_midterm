---
title: "Midterm Report"
author: "Chirag Shah, Nathalie Fadel"
date: "05-14-2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(warning=F)
knitr::opts_chunk$set(message=F)
```

## Exploratory Data Analysis

```{r packages, echo=FALSE, include=FALSE}
library(caret)
library(readxl)
library(tidyverse)
library(glmnet)
library(ISLR)
library(corrplot)
library(pastecs)
library(earth)
library(splines)
library(mgcv)
library(knitr)
library(vip)
library(factoextra)
library(gridExtra)
library(RColorBrewer) 
library(gplots) 
library(ggplot2)
library(igraph)
library(ape)
```

```{r data cleaning, message=F, include=FALSE}
taipei_data <- read_excel("Real_estate_valuation_data_set.xlsx") %>%
  janitor::clean_names()

taipei_data <- na.omit(taipei_data) 
taipei_data$no <- NULL

taipei_data  =
  taipei_data %>%
  rename(house_price = y_house_price_of_unit_area)
taipei_data  = 
 taipei_data %>%
  rename(transaction_date = x1_transaction_date,
         house_age = x2_house_age,
         distance_mrt = x3_distance_to_the_nearest_mrt_station,
         conv_stores = x4_number_of_convenience_stores,
         latitude = x5_latitude,
         longitude = x6_longitude
         )
```

####Descriptive Statistics
```{r, echo=FALSE}
stat.desc(taipei_data) %>% round() %>%  kable(full_width = F, font_size=8)
```


```{r, include=FALSE}
### Designating the Predictors and Outcome
taipei_data <- na.omit(taipei_data) 
taipei_data$no <- NULL

x <- model.matrix(house_price~. ,taipei_data)[,-1]
y <- taipei_data$house_price
```

```{r, echo=FALSE, include=FALSE}
###Observing Correlation of Predictors
corrplot(cor(x))
```


###Scatterplot
```{r, echo=FALSE}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(4, 2))
```

## Models
### Summary of Models Used
Model | R^2 | RMSE 
------------- | ------------- | ------------- 
GAM  | 0.687 | N/A
MARS | 0.7005081 | 7.568432 
KNN | 0.6545951 | 8.208905 
Ridge Regression | 0.599 | 8.799873 
Lasso | 0.599 | 8.810508 
Linear Regression | 0.5824 | 8.782313 

#### Partitioning the Data Set 

```{r, echo=FALSE, message=F}
#Partitioning the dataset
data(taipei_data)

## 75% of the sample size
smp_size <- floor(0.80 * nrow(taipei_data))

## set the seed to make your partition reproducible
set.seed(123)
train_taipei <- sample(seq_len(nrow(taipei_data)), size = smp_size)
train <- taipei_data[train_taipei, ]
test <- taipei_data[-train_taipei, ]
```

#### GAM

```{r GAM calc, echo=FALSE, include=FALSE, message=F}
gam.m1 <- gam(house_price ~ transaction_date + house_age + distance_mrt + conv_stores + latitude + longitude, data = taipei_data)

gam.m2 <- gam(house_price ~ transaction_date + house_age + s(distance_mrt) + conv_stores + latitude + longitude, data = taipei_data)

#Spline term applied to distance to mrt station

gam.m3 <- gam(house_price ~ transaction_date + s(house_age) + distance_mrt + conv_stores + latitude + longitude, data = taipei_data)

#spline term applied to house age

gam.m4 <- gam(house_price ~ transaction_date + s(house_age) + s(distance_mrt) + conv_stores + latitude + longitude, data = taipei_data)

#Both house age and distance to mrt station are splined
```

```{r, echo=FALSE, include=FALSE}
anova(gam.m1, gam.m2, gam.m3, gam.m4, test = "F")
#summary(gam.m2)
#summary(gam.m3)
#summary(gam.m4)


plot(gam.m2)
plot(gam.m3)
```

##### MARS

```{r, echo=FALSE, fig.width=6, fig.height=4}
set.seed(2)
mars_grid <- expand.grid(degree = 1:2, nprune = 2:20)
ctrl1 <- trainControl(method = "cv", number = 10)

mars.fit <- train(x, y, method = "earth", tuneGrid = mars_grid, trControl = ctrl1)

ggplot(mars.fit)
```

```{r, include=FALSE}
print(mars.fit$bestTune)
print(coef(mars.fit$finalModel))
print(mars.fit)
```

```{r marspredictors, echo=FALSE, include=FALSE}
p1 <- vip(mars.fit, num_features = 10, bar = FALSE, value = "gcv") + ggtitle("GCV")
p2 <- vip(mars.fit, num_features = 10, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p1, p2, ncol = 2)
``` 

##### KNN

```{r, include=FALSE}

trainX <- train[,names(train) != "house_price"]
preProcValues <- preProcess(x = trainX,method = c("center", "scale"))
preProcValues
set.seed(1)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
knn_fit <- train(house_price ~., data = train, method = "knn",
            trControl = trctrl,
            preProcess = c("center", "scale"),
            tuneLength = 10)
knn_fit
```
```{r, echo=FALSE, include=TRUE, fig.width=4, fig.height=4}
# Plot model error RMSE vs different values of k
ggplot(knn_fit)
```
```{r, include=FALSE}

# Best tuning parameter k that minimizes the RMSE
knn_fit$bestTune
# Make predictions on the test data
knn_predict <- knn_fit %>% predict(test)

# Compute the prediction error RMSE
RMSE(knn_predict, test$house_price)
```

##### Ridge Regression

```{r, echo=FALSE, include=FALSE}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(123)
ridge.fit <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-1, 10, length = 100))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)
plot(ridge.fit, xTrans = function(x) log(x))
ridge.fit$bestTune
coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)
```
```{r, echo=FALSE, include=FALSE}
p111 <- vip(ridge.fit, num_features = 10, bar = FALSE, value = "gcv") + ggtitle("GCV")
p211 <- vip(ridge.fit, num_features = 10, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p111, p211, ncol = 2)
``` 

```{r, echo=FALSE, include=FALSE}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(123)
ridge.fit2 <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-1, 10, length = 100))),
                     preProc = c("center", "scale"),
                     trControl = ctrl1)
plot(ridge.fit2, xTrans = function(x) log(x))
ridge.fit2$bestTune
coef(ridge.fit2$finalModel,ridge.fit2$bestTune$lambda)
```
```{r ridgereg, echo=FALSE, include=FALSE}
p111_sc <- vip(ridge.fit2, num_features = 10, bar = FALSE, value = "gcv") + ggtitle("GCV")
p211_sc <- vip(ridge.fit2, num_features = 10, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p111_sc, p211_sc, ncol = 2)
``` 

##### Lasso

```{r, include=FALSE}
set.seed(123)
lasso.fit <- train(x, y, method = "glmnet", tuneGrid = expand.grid(alpha = 1, lambda = exp(seq(-5, 5, length=100))), trControl = ctrl1)

plot(lasso.fit, xTrans = function(x) log(x))

lasso.fit$bestTune

coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```

```{r, echo=FALSE, fig.width=4, fig.height=4}
plot(lasso.fit, xTrans = function(x) log(x))
```

##### Linear

```{r, echo=FALSE, include=FALSE}
set.seed(2)
lm.fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)
summary(lm.fit)
```

```{r lmpred, echo=FALSE, include=FALSE}
p11 <- vip(lm.fit, num_features = 10, bar = FALSE, value = "gcv") + ggtitle("GCV")
p21 <- vip(lm.fit, num_features = 10, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p11, p21, ncol = 2)
``` 

```{r calculations, include=FALSE}
sqrt(mean(residuals(lm.fit)^2))
summary(gam.m4)
summary(mars.fit)
sqrt(mean(residuals(ridge.fit)^2))
sqrt(mean(residuals(lasso.fit)^2))
```

#####Clustering

```{r}
scaling <- function(x)
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
clust.scaled <- taipei_data %>%
    map_df(scaling) %>%
    dist(method = 'euclidean') %>%
    hclust(method = 'complete')
clust.scaled$labels <- row.names(taipei_data)[clust.scaled$order]
plot(as.phylo(clust.scaled), type = 'phylogram', 
     tip.color = brewer.pal(3, 'Accent')[cutree(clust.scaled, 3)],
     edge.color = 'steelblue', edge.lty = 2,
     cex = 0.3)
```

```{r}
cut.obs <- cutree(clust.scaled, 3)
#Cluster 1
sc_clust1 <- taipei_data[cut.obs == 1,]
#Cluster 2
sc_clust2 <- taipei_data[cut.obs == 2,]
#Cluster 3
sc_clust3 <- taipei_data[cut.obs == 3,]

sc_clust1
sc_clust2
sc_clust3
```

```{r}
#understanding the differences in each cluster
summary(sc_clust1)
summary(sc_clust2)
summary(sc_clust3)
```

```{r}
scaling <- function(x)
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
clust.scaled <- taipei_data %>%
    map_df(scaling) %>%
    dist(method = 'euclidean') %>%
    hclust(method = 'complete')
clust.scaled$labels <- row.names(taipei_data)[clust.scaled$order]
plot(as.phylo(clust.scaled), type = 'phylogram', 
     tip.color = brewer.pal(2, 'Accent')[cutree(clust.scaled, 2)],
     edge.color = 'steelblue', edge.lty = 2,
     cex = 0.3)
```

```{r}
cut.obs2 <- cutree(clust.scaled, 2)
#Cluster 1
sc_clust4 <- taipei_data[cut.obs2 == 1,]
#Cluster 2
sc_clust5 <- taipei_data[cut.obs2 == 2,]

sc_clust4
sc_clust5
```

```{r}
#understanding the differences in each cluster
summary(sc_clust4)
summary(sc_clust5)
```

## Conclusion
```{r, include=FALSE}
#Comparison using MSE
resamp <- resamples(list(lasso = lasso.fit, ridge = ridge.fit, lm = lm.fit, knn = knn_fit))
summary(resamp)
parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")
```
